{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c03abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webスクレイピングに最低限必要なライブラリをインポート\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "from collections import deque # 高速に先頭からデータを取り出す(pop)ために使います\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268121b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ステータスコード:200\n"
     ]
    }
   ],
   "source": [
    "# アクセスしたいWebサイトのURLを指定\n",
    "url = 'https://github.com/orgs/google/repositories'\n",
    "\n",
    "# WebサーバーにHTTPリクエストを送信\n",
    "# レスポンスを変数に格納しておく\n",
    "res = requests.get(url)\n",
    "\n",
    "print(f\"ステータスコード:{res.status_code}\") # ステータスコードを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b1323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1件目] Accessing: https://github.com/orgs/google/repositories\n",
      "Found 325 new URLs on https://github.com/orgs/google/repositories\n",
      "[2件目] Accessing: https://github.com/\n",
      "Found 25 new URLs on https://github.com/\n",
      "[3件目] Accessing: https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Forgs%2Fgoogle%2Frepositories\n",
      "Found 2 new URLs on https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Forgs%2Fgoogle%2Frepositories\n",
      "[4件目] Accessing: https://github.com/google\n",
      "Found 46 new URLs on https://github.com/google\n",
      "[5件目] Accessing: https://github.com/features/copilot\n",
      "Found 30 new URLs on https://github.com/features/copilot\n",
      "[6件目] Accessing: https://github.com/features/spark\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m res = requests.get(current_url)\n\u001b[32m     20\u001b[39m res.encoding = res.apparent_encoding \u001b[38;5;66;03m# 文字化け対策\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# サーバーに負荷をかけないように0.5秒待機\u001b[39;00m\n\u001b[32m     23\u001b[39m content_type = res.headers.get(\u001b[33m'\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtext/html\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m content_type:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# page_data = {}\n",
    "# visited_url = set()\n",
    "# visit_url_queue = deque([url])\n",
    "# base_domain = urlparse(url).netloc\n",
    "\n",
    "\n",
    "# while visit_url_queue:\n",
    "#     current_url = visit_url_queue.popleft()\n",
    "#     if current_url in visited_url:\n",
    "#         continue\n",
    "\n",
    "#     visited_url.add(current_url)\n",
    "\n",
    "#     # ★修正箇所★\n",
    "#     # どのページを見ているか表示（件数を追加）\n",
    "#     print(f\"[{len(visited_url)}件目] Accessing: {current_url}\")\n",
    "\n",
    "#     try:\n",
    "#         res = requests.get(current_url)\n",
    "#         res.encoding = res.apparent_encoding # 文字化け対策\n",
    "#         time.sleep(1)  # サーバーに負荷をかけないように0.5秒待機\n",
    "\n",
    "#         content_type = res.headers.get('Content-Type', '')\n",
    "#         if 'text/html' not in content_type:\n",
    "#             print(f\"  -> スキップ (Content-Type: {content_type})\")\n",
    "#             page_data[current_url] = f\"HTMLではない ({content_type})\"\n",
    "#             continue # このページの処理を中断\n",
    "\n",
    "#         soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "#         title_tag = soup.title\n",
    "#         if title_tag and title_tag.string:\n",
    "#             title_text = title_tag.string.strip() # .string で中身を取得し、strip()で前後の空白削除\n",
    "#         else:\n",
    "#             title_text = 'タイトルなし' # titleタグがない、または中身が空の場合\n",
    "        \n",
    "#         page_data[current_url] = title_text # 辞書に格納\n",
    "\n",
    "#         links = soup.select('a[href]')\n",
    "\n",
    "#         found_count = 0\n",
    "#         for link in links:\n",
    "#             new_url = urljoin(current_url, link['href'])\n",
    "\n",
    "#             new_url = new_url.split('#')[0]\n",
    "\n",
    "#             parsed_new_url = urlparse(new_url)\n",
    "\n",
    "#             if parsed_new_url.scheme not in ['http', 'https']:\n",
    "#                 continue\n",
    "\n",
    "\n",
    "\n",
    "#             if parsed_new_url.netloc == base_domain:\n",
    "#                 if new_url not in visited_url and new_url not in visit_url_queue:\n",
    "#                     visit_url_queue.append(new_url)\n",
    "#                     found_count += 1\n",
    "\n",
    "\n",
    "#         print(f\"Found {found_count} new URLs on {current_url}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error accessing {current_url}: {e}\")\n",
    "\n",
    "# print(\"\\n--- クローリング終了 ---\")\n",
    "# print(f\"訪問した総ページ数: {len(visited_url)}\")\n",
    "\n",
    "# # -----------------------------------\n",
    "# # 課題対応：最終的な辞書を表示\n",
    "# # -----------------------------------\n",
    "# print(\"\\n--- 取得した URL と タイトル の辞書 ---\")\n",
    "# # 件数が多いと見にくいため、pprint を使うとより見やすくなります\n",
    "# # from pprint import pprint\n",
    "# # pprint(page_data)\n",
    "# print(page_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://github.com/features/spark\n",
      "GitHub Copilot\n",
      "GitHub Spark\n",
      "GitHub Models\n",
      "Try it now\n",
      "Try GitHub Spark\n",
      "Get started\n",
      "Get started\n",
      "GitHub on LinkedIn\n",
      "GitHub on Instagram\n",
      "GitHub on YouTube\n",
      "GitHub on X\n",
      "GitHub on TikTok\n",
      "GitHub on Twitch\n",
      "GitHub’s organization on GitHub\n",
      "English\n",
      "Português (Brasil)\n",
      "Español (América Latina)\n",
      "日本語\n",
      "한국어\n",
      "Finished scraping.\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while current_url:\n",
    "    print(f\"Fetching: {current_url}\")\n",
    "    \n",
    "    # ⭐ ここで遅延を入れるのが正しい配置です ⭐\n",
    "    time.sleep(2) # 例：2秒待機 (負荷軽減のため必須)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(current_url)\n",
    "        response.raise_for_status() # 200 OK以外はエラーにする\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 特定のタグをすべて抽出する（例：aタグ）\n",
    "        names = soup.select('a > span')\n",
    "\n",
    "        # 結果は Beautiful Soup の Tag オブジェクトのリストになります\n",
    "        for name_span in names:\n",
    "            print(name_span.text.strip())\n",
    "        \n",
    "        # 処理が進むように今回はNoneで終了させています\n",
    "        current_url = None \n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request Error: {e}\")\n",
    "        break\n",
    "\n",
    "print(\"Finished scraping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c62b12c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://github.com/google?tab=repositories\n",
      "Found 10 repository list items (li).\n",
      "Finished scraping.\n",
      "------------------------------\n",
      "--- 取得結果（最初の15件）---\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'C++', 'スターの数': 4984}\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'C++', 'スターの数': 608}\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'Python', 'スターの数': 166}\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'C++', 'スターの数': 3581}\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'JavaScript', 'スターの数': 2867}\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'C++', 'スターの数': 10266}\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'Go', 'スターの数': 158}\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'Java', 'スターの数': 1767}\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'Java', 'スターの数': 58}\n",
      "{'リポジトリ名': 'N/A', '主要な言語': 'C++', 'スターの数': 571}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3 # データベース処理のために追加\n",
    "\n",
    "# --- 1. 初期化と初期URL設定 ---\n",
    "current_url = \"/google?tab=repositories\" \n",
    "all_repositories = [] \n",
    "\n",
    "# --- 2. ページネーションを考慮したループ ---\n",
    "while current_url:\n",
    "    full_url = \"https://github.com\" + current_url \n",
    "    print(f\"Fetching: {full_url}\")\n",
    "    \n",
    "    # 課題の要件に合わせて time.sleep(1) に修正\n",
    "    time.sleep(1) \n",
    "    \n",
    "    try:\n",
    "        response = requests.get(full_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # ✅ 修正点1: リポジトリの塊のセレクタをより具体的に指定\n",
    "        # GitHubの組織リポジトリリストの項目を指すセレクタを使用\n",
    "        # Googleのページ構造に合わせ、`<div id=\"org-repositories\">` の中の `li` を狙います。\n",
    "        repo_items = soup.select('#org-repositories li') \n",
    "        print(f\"Found {len(repo_items)} repository list items (li).\") \n",
    "        \n",
    "        # 実際にリポジトリ情報が入っている項目のみを処理\n",
    "        for item in repo_items:\n",
    "            # 1. リポジトリ名の抽出\n",
    "            # ✅ 修正点2: `<h3>` タグの中の `<a>` タグを狙う\n",
    "            # `h3` の中の `a` タグのテキストがリポジトリ名になっています\n",
    "            name_tag = item.select_one('h3 a') \n",
    "            repo_name = name_tag.text.strip() if name_tag else \"N/A\"\n",
    "            \n",
    "            # 2. 主要言語の抽出\n",
    "            # ✅ 修正点3: 言語名を示す `span` を狙う\n",
    "            # 言語名は `span[itemprop=\"programmingLanguage\"]` のテキストとして格納されています\n",
    "            language_tag = item.select_one('span[itemprop=\"programmingLanguage\"]')\n",
    "            language = language_tag.text.strip() if language_tag else \"N/A\"\n",
    "            \n",
    "            # 3. スターの数の抽出\n",
    "            # ✅ 修正点4: スターの数を保持している `a` タグを狙う\n",
    "            # スター数は、href属性が `/stargazers` で終わる `a` タグのテキストです。\n",
    "            star_tag = item.select_one('a[href*=\"/stargazers\"]')\n",
    "            star_count = star_tag.text.strip() if star_tag else \"0\"\n",
    "            \n",
    "            # カンマや k を処理\n",
    "            if 'k' in star_count:\n",
    "                 # k の処理（例: '1.2k' -> '1200'）\n",
    "                if '.' in star_count:\n",
    "                    # '1.2k' のように小数点がある場合\n",
    "                    num_str = star_count.replace('k', '').replace(',', '')\n",
    "                    star_count = str(int(float(num_str) * 1000))\n",
    "                else:\n",
    "                    # '1k' のように小数点がない場合\n",
    "                    star_count = star_count.replace('k', '000').replace(',', '')\n",
    "            else:\n",
    "                 # カンマだけを削除\n",
    "                star_count = star_count.replace(',', '')\n",
    "            \n",
    "            # データ型をintに変換できるか確認（文字列として格納するために不要ならスキップ）\n",
    "            # DBに格納する際にINT型にするために、ここで数値に変換します\n",
    "            try:\n",
    "                star_count = int(star_count)\n",
    "            except ValueError:\n",
    "                star_count = 0 # 変換できない場合は 0 \n",
    "            \n",
    "            all_repositories.append({\n",
    "                'リポジトリ名': repo_name,\n",
    "                '主要な言語': language,\n",
    "                'スターの数': star_count \n",
    "            })\n",
    "\n",
    "        # ページネーションのロジックはそのまま\n",
    "        next_link = soup.select_one('a.next_page') # 次ページボタンのセレクタもより具体的になりました\n",
    "        if next_link and next_link.has_attr('href'):\n",
    "            current_url = next_link['href']\n",
    "        else:\n",
    "            current_url = None \n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request Error: {e}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "print(\"Finished scraping.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 最終結果の表示（一部）\n",
    "print(\"--- 取得結果（最初の15件）---\")\n",
    "for i, repo in enumerate(all_repositories[:15]): \n",
    "    print(repo)\n",
    "    if i == 14 and len(all_repositories) > 15:\n",
    "        print('...(以下省略)...')\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbb695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d7d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
